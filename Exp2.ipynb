{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPM+cVWDLLi6vIjdoTqXV67",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsyashkhurana/Big-Data-Analystics/blob/main/Exp2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhnuQSQz_Ji8",
        "outputId": "89affb15-ed4c-4b9c-c7e3-6d11e6a402e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (108.157.173.89)] [Connecting to r2u.stat\r                                                                                                    \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (108.157.173.89)] [Connected to r2u.stat.\r                                                                                                    \rHit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (108.157.173.89)] [Connected to r2u.stat.\r                                                                                                    \rHit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] [Connected to develop\r                                                                                                    \rHit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-8-jdk is already the newest version (8u442-b06~us1-0ubuntu1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "--2025-03-08 09:13:56--  https://downloads.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f8:10a:39da::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-03-08 09:13:56 ERROR 404: Not Found.\n",
            "\n",
            "tar (child): hadoop-3.3.1.tar.gz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "mv: cannot stat '/usr/local/hadoop-3.3.1': No such file or directory\n",
            "/bin/bash: line 1: hdfs: command not found\n",
            "/bin/bash: line 1: start-dfs.sh: command not found\n",
            "/bin/bash: line 1: hdfs: command not found\n",
            "/bin/bash: line 1: hdfs: command not found\n",
            "/bin/bash: line 1: hdfs: command not found\n",
            "Hello, Hadoop! This is Yash\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "!apt-get update -y\n",
        "!apt-get install openjdk-8-jdk -y\n",
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.tar.gz\n",
        "!tar -xvzf hadoop-3.3.1.tar.gz -C /usr/local/\n",
        "!mv /usr/local/hadoop-3.3.1 /usr/local/hadoop\n",
        "\n",
        "# Ensure configuration directory exists\n",
        "hadoop_conf_dir = \"/usr/local/hadoop/etc/hadoop\"\n",
        "os.makedirs(hadoop_conf_dir, exist_ok=True)\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['HADOOP_HOME'] = '/usr/local/hadoop'\n",
        "os.environ['PATH'] += ':/usr/local/hadoop/bin:/usr/local/hadoop/sbin'\n",
        "\n",
        "# Configure core-site.xml\n",
        "core_site_config = \"\"\"<?xml version=\"1.0\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>fs.defaultFS</name>\n",
        "        <value>hdfs://localhost:9000</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "\"\"\"\n",
        "with open('/usr/local/hadoop/etc/hadoop/core-site.xml', 'w') as f:\n",
        "    f.write(core_site_config)\n",
        "\n",
        "# Configure hdfs-site.xml\n",
        "hdfs_site_config = \"\"\"<?xml version=\"1.0\"?>\n",
        "<configuration>\n",
        "    <property>\n",
        "        <name>dfs.replication</name>\n",
        "        <value>1</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>dfs.name.dir</name>\n",
        "        <value>/usr/local/hadoop/hdfs/namenode</value>\n",
        "    </property>\n",
        "    <property>\n",
        "        <name>dfs.data.dir</name>\n",
        "        <value>/usr/local/hadoop/hdfs/datanode</value>\n",
        "    </property>\n",
        "</configuration>\n",
        "\"\"\"\n",
        "with open('/usr/local/hadoop/etc/hadoop/hdfs-site.xml', 'w') as f:\n",
        "    f.write(hdfs_site_config)\n",
        "# Format HDFS\n",
        "!hdfs namenode -format\n",
        "\n",
        "# Start Hadoop services\n",
        "!start-dfs.sh\n",
        "\n",
        "# Create directory in HDFS\n",
        "!hdfs dfs -mkdir /mydata\n",
        "# Create a sample text file and upload to HDFS\n",
        "!echo \"Hello, Hadoop! This is Yash\" > localfile.txt\n",
        "!hdfs dfs -put localfile.txt /mydata\n",
        "\n",
        "# Retrieve the file from HDFS\n",
        "!hdfs dfs -get /mydata/localfile.txt .\n",
        "\n",
        "# Display the retrieved file\n",
        "!cat localfile.txt"
      ]
    }
  ]
}